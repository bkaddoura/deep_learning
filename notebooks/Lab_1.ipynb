{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks with Keras\n",
    "\n",
    "Welcome to the first practical session of the course! In this session, we will learn how to train neural networks with Keras. We will start with a simple example of a feedforward neural network for classification and then we will study the impact of the initialization of the weights on the convergence of the training algorithm.\n",
    "\n",
    "Keras is a high-level neural network API, built on top of TensorFlow 2.0. It provides a user-friendly interface to build, train and deploy deep learning models. Keras is designed to be modular, fast and easy to use.\n",
    "\n",
    "Throughout this course, we will focus on using Keras and TensorFlow for building and training neural networks. However, there are other popular deep learning frameworks such as PyTorch, MXNet, CNTK, etc. that you can also use to build and train neural networks.\n",
    "\n",
    "In order to use our code on Google Colab, we will need to ensure that any required packages are installed. We will use the following packages in this session:\n",
    "\n",
    "- `tensorflow`: an open-source library for numerical computation and large-scale machine learning.\n",
    "- `matplotlib`: a plotting library for the Python programming language and its numerical mathematics extension NumPy.\n",
    "- `numpy`: a library for scientific computing in Python.\n",
    "- `sklearn`: a machine learning library for the Python programming language.\n",
    "- `pandas`: a library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "The following cell will check if the packages are installed, and if not, it will install them. Students familiar with how pip works might note that it already checks this before installing! The reason for this code (which will also appear in subsequent notebooks) is to speed up execution if you re-run the entire notebook - it will skip the installation step if the packages are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached grpcio-1.62.1-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading keras-3.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading optree-0.11.0-cp311-cp311-win_amd64.whl.metadata (46 kB)\n",
      "     ---------------------------------------- 0.0/46.2 kB ? eta -:--:--\n",
      "     ---------------- --------------------- 20.5/46.2 kB 330.3 kB/s eta 0:00:01\n",
      "     ---------------- --------------------- 20.5/46.2 kB 330.3 kB/s eta 0:00:01\n",
      "     --------------------------------- ---- 41.0/46.2 kB 245.8 kB/s eta 0:00:01\n",
      "     --------------------------------- ---- 41.0/46.2 kB 245.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 46.2/46.2 kB 192.4 kB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.16.1-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Using cached tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl (377.0 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.62.1-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.0 MB 660.6 kB/s eta 0:00:05\n",
      "    --------------------------------------- 0.0/3.0 MB 393.8 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/3.0 MB 393.8 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.1/3.0 MB 459.5 kB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.1/3.0 MB 403.5 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.1/3.0 MB 409.6 kB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.1/3.0 MB 426.7 kB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.2/3.0 MB 456.4 kB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.2/3.0 MB 456.4 kB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.2/3.0 MB 445.2 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.2/3.0 MB 458.5 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.3/3.0 MB 449.3 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.3/3.0 MB 465.5 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 0.3/3.0 MB 464.0 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 0.3/3.0 MB 487.6 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.4/3.0 MB 516.2 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.4/3.0 MB 524.5 kB/s eta 0:00:05\n",
      "   ------ --------------------------------- 0.5/3.0 MB 542.3 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.5/3.0 MB 585.1 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.5/3.0 MB 586.8 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.6/3.0 MB 581.2 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.6/3.0 MB 582.8 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.6/3.0 MB 560.4 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.7/3.0 MB 589.7 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.7/3.0 MB 589.7 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.7/3.0 MB 544.8 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.7/3.0 MB 544.8 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.7/3.0 MB 521.1 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.7/3.0 MB 521.1 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.7/3.0 MB 508.3 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.7/3.0 MB 508.3 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.7/3.0 MB 491.5 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.7/3.0 MB 491.5 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 480.2 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 497.8 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.8/3.0 MB 490.0 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.8/3.0 MB 488.5 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.9/3.0 MB 500.2 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 1.0/3.0 MB 532.2 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 1.0/3.0 MB 556.3 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 1.1/3.0 MB 558.7 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 1.1/3.0 MB 555.8 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 1.2/3.0 MB 579.3 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 1.2/3.0 MB 591.0 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.3/3.0 MB 597.2 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.3/3.0 MB 607.9 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.4/3.0 MB 608.9 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.4/3.0 MB 623.0 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.4/3.0 MB 619.5 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.5/3.0 MB 624.7 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.5/3.0 MB 620.9 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.5/3.0 MB 620.9 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 1.6/3.0 MB 626.3 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 1.6/3.0 MB 627.1 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 1.6/3.0 MB 623.6 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 1.6/3.0 MB 620.6 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 1.6/3.0 MB 620.6 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 1.7/3.0 MB 617.4 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 1.7/3.0 MB 607.0 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 1.7/3.0 MB 604.1 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.7/3.0 MB 604.7 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.8/3.0 MB 599.0 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.8/3.0 MB 596.7 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 1.8/3.0 MB 597.3 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.8/3.0 MB 598.2 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.9/3.0 MB 596.2 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.9/3.0 MB 596.8 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.9/3.0 MB 597.8 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.9/3.0 MB 598.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.0/3.0 MB 599.2 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.0/3.0 MB 603.3 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.0/3.0 MB 601.1 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.1/3.0 MB 601.8 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 599.2 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 600.3 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 2.2/3.0 MB 601.0 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 2.2/3.0 MB 601.7 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.2/3.0 MB 607.9 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.3/3.0 MB 603.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.3/3.0 MB 606.1 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.3/3.0 MB 604.1 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 2.3/3.0 MB 602.4 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 2.4/3.0 MB 603.1 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 2.4/3.0 MB 606.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.5/3.0 MB 611.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.5/3.0 MB 612.6 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.5/3.0 MB 618.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.6/3.0 MB 618.4 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 621.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 621.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.7/3.0 MB 614.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 622.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 622.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 622.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 618.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 621.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.8/3.0 MB 619.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.9/3.0 MB 619.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/3.0 MB 620.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 627.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 627.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 627.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 627.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 627.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 627.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 627.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 627.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 627.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 579.3 kB/s eta 0:00:00\n",
      "Downloading keras-3.2.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.1/1.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.2/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.2/1.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.3/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.4/1.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.5/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.5/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.6/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.6/1.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.6/1.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.7/1.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.7/1.1 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.1 MB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.7/1.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.8/1.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.9/1.1 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 0.9/1.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 0.9/1.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.0/1.1 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.0/1.1 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 963.6 kB/s eta 0:00:00\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl (127 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.8 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 41.0/66.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 61.4/66.8 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 61.4/66.8 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 61.4/66.8 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 61.4/66.8 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 61.4/66.8 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 61.4/66.8 kB 656.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 66.8/66.8 kB 172.5 kB/s eta 0:00:00\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Downloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "   ---------------------------------------- 0.0/226.8 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 143.4/226.8 kB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 174.1/226.8 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  225.3/226.8 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  225.3/226.8 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  225.3/226.8 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- 226.8/226.8 kB 866.4 kB/s eta 0:00:00\n",
      "Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Using cached namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp311-cp311-win_amd64.whl (245 kB)\n",
      "   ---------------------------------------- 0.0/245.0 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/245.0 kB 660.6 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 71.7/245.0 kB 991.0 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 71.7/245.0 kB 991.0 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 92.2/245.0 kB 525.1 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 143.4/245.0 kB 711.9 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 153.6/245.0 kB 541.0 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 184.3/245.0 kB 586.1 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 235.5/245.0 kB 627.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 235.5/245.0 kB 627.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 245.0/245.0 kB 537.2 kB/s eta 0:00:00\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, absl-py, werkzeug, requests, opt-einsum, ml-dtypes, markdown-it-py, h5py, astunparse, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 certifi-2024.2.2 charset-normalizer-3.3.2 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.11.0 idna-3.7 keras-3.2.1 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.7 numpy-1.26.4 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 requests-2.31.0 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 urllib3-2.2.1 werkzeug-3.0.2 wheel-0.43.0 wrapt-1.16.0\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.4-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp311-cp311-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.51.0-cp311-cp311-win_amd64.whl.metadata (162 kB)\n",
      "     ---------------------------------------- 0.0/162.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/162.8 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/162.8 kB ? eta -:--:--\n",
      "     --------- --------------------------- 41.0/162.8 kB 991.0 kB/s eta 0:00:01\n",
      "     ------------- ----------------------- 61.4/162.8 kB 656.4 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 81.9/162.8 kB 573.4 kB/s eta 0:00:01\n",
      "     -------------------- ---------------- 92.2/162.8 kB 581.0 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 112.6/162.8 kB 467.6 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 112.6/162.8 kB 467.6 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 112.6/162.8 kB 467.6 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 153.6/162.8 kB 382.3 kB/s eta 0:00:01\n",
      "     ------------------------------------- 162.8/162.8 kB 61.0 kB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.5-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.3.0-cp311-cp311-win_amd64.whl.metadata (9.4 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.4-cp311-cp311-win_amd64.whl (7.7 MB)\n",
      "   ---------------------------------------- 0.0/7.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.7 MB 660.6 kB/s eta 0:00:12\n",
      "   ---------------------------------------- 0.0/7.7 MB 393.8 kB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.1/7.7 MB 409.6 kB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.1/7.7 MB 416.7 kB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.1/7.7 MB 374.1 kB/s eta 0:00:21\n",
      "    --------------------------------------- 0.1/7.7 MB 409.6 kB/s eta 0:00:19\n",
      "    --------------------------------------- 0.1/7.7 MB 387.0 kB/s eta 0:00:20\n",
      "    --------------------------------------- 0.2/7.7 MB 392.8 kB/s eta 0:00:20\n",
      "    --------------------------------------- 0.2/7.7 MB 388.2 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.2/7.7 MB 380.8 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.2/7.7 MB 377.1 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.2/7.7 MB 393.1 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.3/7.7 MB 393.2 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.3/7.7 MB 405.9 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.3/7.7 MB 413.0 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.3/7.7 MB 423.3 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.3/7.7 MB 419.6 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.4/7.7 MB 417.2 kB/s eta 0:00:18\n",
      "   -- ------------------------------------- 0.4/7.7 MB 440.8 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.4/7.7 MB 443.8 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.5/7.7 MB 447.5 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.5/7.7 MB 459.7 kB/s eta 0:00:16\n",
      "   -- ------------------------------------- 0.5/7.7 MB 449.3 kB/s eta 0:00:16\n",
      "   -- ------------------------------------- 0.5/7.7 MB 451.8 kB/s eta 0:00:16\n",
      "   -- ------------------------------------- 0.6/7.7 MB 468.2 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 0.6/7.7 MB 475.1 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.6/7.7 MB 465.8 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 0.7/7.7 MB 480.0 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.7/7.7 MB 485.9 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.7/7.7 MB 491.3 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.7/7.7 MB 489.9 kB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 0.8/7.7 MB 494.9 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 0.8/7.7 MB 513.9 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 0.9/7.7 MB 549.1 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.0/7.7 MB 564.2 kB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 1.0/7.7 MB 589.7 kB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 1.1/7.7 MB 603.0 kB/s eta 0:00:11\n",
      "   ------ --------------------------------- 1.2/7.7 MB 632.6 kB/s eta 0:00:11\n",
      "   ------ --------------------------------- 1.2/7.7 MB 649.7 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.3/7.7 MB 655.4 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.3/7.7 MB 671.1 kB/s eta 0:00:10\n",
      "   ------- -------------------------------- 1.4/7.7 MB 685.7 kB/s eta 0:00:10\n",
      "   ------- -------------------------------- 1.5/7.7 MB 699.9 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 1.5/7.7 MB 708.9 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 1.6/7.7 MB 731.4 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 1.6/7.7 MB 739.1 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 1.7/7.7 MB 756.3 kB/s eta 0:00:08\n",
      "   --------- ------------------------------ 1.8/7.7 MB 768.2 kB/s eta 0:00:08\n",
      "   --------- ------------------------------ 1.8/7.7 MB 755.8 kB/s eta 0:00:08\n",
      "   --------- ------------------------------ 1.9/7.7 MB 775.2 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.0/7.7 MB 802.2 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.0/7.7 MB 802.2 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.0/7.7 MB 805.1 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.1/7.7 MB 802.4 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.1/7.7 MB 811.4 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.2/7.7 MB 816.3 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.2/7.7 MB 821.0 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.3/7.7 MB 825.6 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 2.4/7.7 MB 833.6 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 2.4/7.7 MB 848.9 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 2.5/7.7 MB 856.3 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 2.6/7.7 MB 866.9 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.6/7.7 MB 870.4 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.7/7.7 MB 883.9 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.8/7.7 MB 896.9 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.9/7.7 MB 909.6 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 2.9/7.7 MB 915.4 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 3.0/7.7 MB 927.7 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 3.1/7.7 MB 945.7 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 3.2/7.7 MB 944.6 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 3.2/7.7 MB 952.7 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 3.3/7.7 MB 948.6 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 3.3/7.7 MB 959.4 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 3.4/7.7 MB 965.7 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 3.5/7.7 MB 974.3 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 3.6/7.7 MB 981.7 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 3.6/7.7 MB 991.5 kB/s eta 0:00:05\n",
      "   ------------------- -------------------- 3.7/7.7 MB 995.5 kB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.7/7.7 MB 996.7 kB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.7/7.7 MB 980.2 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 3.8/7.7 MB 993.6 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 3.9/7.7 MB 997.4 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 4.0/7.7 MB 999.9 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 4.0/7.7 MB 1.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 4.1/7.7 MB 999.5 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 4.1/7.7 MB 1.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 4.2/7.7 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.2/7.7 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.3/7.7 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.4/7.7 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.4/7.7 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.4/7.7 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.5/7.7 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.5/7.7 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.5/7.7 MB 997.7 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.5/7.7 MB 998.9 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.6/7.7 MB 996.4 kB/s eta 0:00:04\n",
      "   ------------------------ --------------- 4.6/7.7 MB 988.5 kB/s eta 0:00:04\n",
      "   ------------------------ --------------- 4.7/7.7 MB 992.9 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.7/7.7 MB 993.8 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.8/7.7 MB 998.1 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.8/7.7 MB 999.0 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.8/7.7 MB 999.0 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.9/7.7 MB 988.2 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.9/7.7 MB 984.1 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.0/7.7 MB 983.1 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.0/7.7 MB 987.0 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.1/7.7 MB 989.0 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.1/7.7 MB 986.0 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.2/7.7 MB 990.0 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.2/7.7 MB 988.9 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.2/7.7 MB 984.0 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.3/7.7 MB 982.2 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.3/7.7 MB 979.3 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.4/7.7 MB 982.9 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.4/7.7 MB 983.0 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.5/7.7 MB 983.9 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.5/7.7 MB 981.2 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 5.6/7.7 MB 985.7 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 5.6/7.7 MB 988.4 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 5.7/7.7 MB 989.4 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.7/7.7 MB 991.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.7/7.7 MB 991.9 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.8/7.7 MB 984.8 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.8/7.7 MB 979.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.8/7.7 MB 974.4 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.8/7.7 MB 975.4 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.7 MB 964.4 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.7 MB 968.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.0/7.7 MB 969.8 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.0/7.7 MB 967.4 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.1/7.7 MB 968.3 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.1/7.7 MB 970.0 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.2/7.7 MB 968.5 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.2/7.7 MB 967.0 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.2/7.7 MB 964.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.2/7.7 MB 960.2 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.3/7.7 MB 961.1 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.3/7.7 MB 958.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.3/7.7 MB 958.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.3/7.7 MB 950.0 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.4/7.7 MB 948.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.4/7.7 MB 944.4 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.4/7.7 MB 942.4 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.5/7.7 MB 943.5 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 6.5/7.7 MB 941.6 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 6.6/7.7 MB 943.1 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 6.6/7.7 MB 943.5 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 6.6/7.7 MB 942.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 6.7/7.7 MB 945.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.8/7.7 MB 947.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.8/7.7 MB 950.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.9/7.7 MB 951.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.0/7.7 MB 956.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.0/7.7 MB 953.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.7 MB 957.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.7 MB 958.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.7 MB 958.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.7 MB 958.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.7 MB 958.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.7 MB 958.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.7 MB 958.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.7 MB 958.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.7 MB 958.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.7 MB 958.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 971.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.7 MB 973.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.7/7.7 MB 846.9 kB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.1-cp311-cp311-win_amd64.whl (188 kB)\n",
      "   ---------------------------------------- 0.0/188.2 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 41.0/188.2 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 71.7/188.2 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------- ------------------- 92.2/188.2 kB 751.6 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 153.6/188.2 kB 833.5 kB/s eta 0:00:01\n",
      "   -------------------------------------  184.3/188.2 kB 798.5 kB/s eta 0:00:01\n",
      "   -------------------------------------  184.3/188.2 kB 798.5 kB/s eta 0:00:01\n",
      "   -------------------------------------  184.3/188.2 kB 798.5 kB/s eta 0:00:01\n",
      "   -------------------------------------  184.3/188.2 kB 798.5 kB/s eta 0:00:01\n",
      "   -------------------------------------  184.3/188.2 kB 798.5 kB/s eta 0:00:01\n",
      "   -------------------------------------  184.3/188.2 kB 798.5 kB/s eta 0:00:01\n",
      "   -------------------------------------  184.3/188.2 kB 798.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 188.2/188.2 kB 325.3 kB/s eta 0:00:00\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.51.0-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.2 MB 1.3 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.1/2.2 MB 812.7 kB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/2.2 MB 871.5 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/2.2 MB 717.5 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/2.2 MB 708.1 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.2/2.2 MB 655.4 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.2/2.2 MB 655.4 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.2/2.2 MB 535.8 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 578.7 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 578.7 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 578.7 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 578.7 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.3/2.2 MB 436.5 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.3/2.2 MB 436.5 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 396.2 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 451.6 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 451.6 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 451.6 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 451.6 kB/s eta 0:00:05\n",
      "   ------ --------------------------------- 0.3/2.2 MB 367.8 kB/s eta 0:00:06\n",
      "   ------ --------------------------------- 0.3/2.2 MB 367.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.4/2.2 MB 423.3 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.5/2.2 MB 426.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.5/2.2 MB 426.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.5/2.2 MB 426.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.5/2.2 MB 410.6 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.5/2.2 MB 410.6 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.5/2.2 MB 410.6 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.5/2.2 MB 374.3 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.5/2.2 MB 375.4 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 381.7 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 406.8 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 0.7/2.2 MB 427.8 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 0.7/2.2 MB 438.9 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 0.7/2.2 MB 438.9 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.7/2.2 MB 444.9 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.8/2.2 MB 458.7 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.8/2.2 MB 475.5 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 491.4 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.9/2.2 MB 501.0 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 514.4 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.0/2.2 MB 511.5 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.0/2.2 MB 501.8 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 1.1/2.2 MB 538.4 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 1.1/2.2 MB 542.8 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.2/2.2 MB 545.2 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.2/2.2 MB 538.2 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 549.8 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 549.8 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 537.3 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 537.3 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 525.0 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 526.7 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 526.7 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.3/2.2 MB 514.6 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.3/2.2 MB 514.6 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.4/2.2 MB 509.7 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.4/2.2 MB 509.7 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.4/2.2 MB 506.4 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.4/2.2 MB 506.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.4/2.2 MB 500.5 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.4/2.2 MB 500.5 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.4/2.2 MB 500.5 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.5/2.2 MB 493.2 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.5/2.2 MB 494.0 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.5/2.2 MB 493.1 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 498.1 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 504.3 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.6/2.2 MB 506.5 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.7/2.2 MB 514.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 526.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 540.1 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 541.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.2 MB 552.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 565.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 578.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.2 MB 590.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 593.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 593.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 593.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 593.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 564.2 kB/s eta 0:00:00\n",
      "Using cached kiwisolver-1.4.5-cp311-cp311-win_amd64.whl (56 kB)\n",
      "Downloading pillow-10.3.0-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.5 MB 1.9 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.3/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.4/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.6/2.5 MB 1.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.7/2.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.7/2.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.7/2.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.5 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.5 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.9/2.5 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.9/2.5 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.1/2.5 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.2/2.5 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.3/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.4/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.4/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.5/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.6/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.7/2.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.8/2.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.9/2.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.9/2.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.0/2.5 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.2/2.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.2/2.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.2/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.3/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.3/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.3/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.3/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.4/2.5 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 932.7 kB/s eta 0:00:00\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 kiwisolver-1.4.5 matplotlib-3.8.4 pillow-10.3.0 pyparsing-3.1.2\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bash\\documents\\github\\deep_learning\\envdl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB 653.6 kB/s eta 0:00:18\n",
      "   ---------------------------------------- 0.1/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.1/11.6 MB 1.1 MB/s eta 0:00:11\n",
      "    --------------------------------------- 0.2/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.3/11.6 MB 1.3 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.4/11.6 MB 1.3 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.4/11.6 MB 1.2 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.5/11.6 MB 1.1 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.5/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.6/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.6/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.7/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.7/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.7/11.6 MB 1.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.7/11.6 MB 1.1 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 0.9/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 0.9/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.0/11.6 MB 1.1 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.1/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.1/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.1/11.6 MB 1.1 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.2/11.6 MB 1.1 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.2/11.6 MB 1.1 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.2/11.6 MB 1.0 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.2/11.6 MB 1.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 1.2/11.6 MB 1.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 1.2/11.6 MB 970.6 kB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 929.5 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 929.5 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 929.5 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 929.5 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 822.1 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 825.5 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 814.1 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 814.1 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.4/11.6 MB 792.3 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.4/11.6 MB 784.1 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.4/11.6 MB 779.4 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.5/11.6 MB 776.3 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 1.5/11.6 MB 777.9 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 1.5/11.6 MB 780.1 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 782.2 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 779.3 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 779.3 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 759.6 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 1.7/11.6 MB 767.0 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.8/11.6 MB 778.9 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.8/11.6 MB 781.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.8/11.6 MB 782.0 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.9/11.6 MB 779.5 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.9/11.6 MB 785.4 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 2.0/11.6 MB 788.0 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 2.0/11.6 MB 796.7 kB/s eta 0:00:13\n",
      "   ------- -------------------------------- 2.1/11.6 MB 806.3 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.2/11.6 MB 818.1 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.2/11.6 MB 835.4 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.3/11.6 MB 836.1 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.4/11.6 MB 842.9 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.4/11.6 MB 847.3 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.5/11.6 MB 854.8 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.5/11.6 MB 862.1 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.6/11.6 MB 880.8 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.7/11.6 MB 889.5 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.8/11.6 MB 902.6 kB/s eta 0:00:10\n",
      "   --------- ------------------------------ 2.8/11.6 MB 909.3 kB/s eta 0:00:10\n",
      "   --------- ------------------------------ 2.8/11.6 MB 888.7 kB/s eta 0:00:10\n",
      "   --------- ------------------------------ 2.9/11.6 MB 888.8 kB/s eta 0:00:10\n",
      "   --------- ------------------------------ 2.9/11.6 MB 886.3 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 2.9/11.6 MB 885.2 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.0/11.6 MB 886.2 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.0/11.6 MB 883.6 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 892.6 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 889.4 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.2/11.6 MB 892.1 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.3/11.6 MB 903.2 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.3/11.6 MB 916.8 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 931.4 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 3.5/11.6 MB 937.6 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 3.6/11.6 MB 950.4 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 3.7/11.6 MB 964.1 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 3.8/11.6 MB 975.1 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 3.9/11.6 MB 984.3 kB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.0/11.6 MB 993.3 kB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.0/11.6 MB 1.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.1/11.6 MB 999.4 kB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.2/11.6 MB 1.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.2/11.6 MB 1.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.3/11.6 MB 1.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 4.4/11.6 MB 1.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 4.5/11.6 MB 1.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 4.5/11.6 MB 1.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 4.6/11.6 MB 1.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 4.6/11.6 MB 1.0 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 4.7/11.6 MB 1.0 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 4.8/11.6 MB 1.0 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 4.8/11.6 MB 1.0 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 4.8/11.6 MB 1.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 4.9/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.0/11.6 MB 1.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.0/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.1/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.2/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 5.2/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 5.4/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.4/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.5/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.6/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.6/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.7/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 5.8/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 5.9/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.0/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.0/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.1/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 6.2/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.2/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.3/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.4/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.5/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.6/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.7/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.7/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.9/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.9/11.6 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.9/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.0/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.0/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.1/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.2/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.2/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.2/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.2/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.4/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.4/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.5/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.5/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.5/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.6/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.6/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.7/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.7/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.8/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.8/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.9/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.9/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.0/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.0/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.2/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.2/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.3/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.3/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.4/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.4/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.4/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.5/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.5/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.6/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.6/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.8/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.8/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.9/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.0/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.0/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.1/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.1/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.2/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.2/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.3/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.3/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.3/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.4/11.6 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.5/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.5/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.6/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.6/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.6/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.7/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.7/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.8/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.8/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.9/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.9/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.0/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.1/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.1/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.2/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.2/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.3/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.4/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.4/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.4/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.5/11.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.5/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.6/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.6/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.7/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.7/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.8/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.8/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.9/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.9/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.2/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 1.2 MB/s eta 0:00:00\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "# Check if the packages are installed, if not install them.\n",
    "# Note - if you are working locally, you may want to comment this section out\n",
    "# ...and use your preferred method of installing packages.\n",
    "import importlib\n",
    "\n",
    "def install_if_missing(package):\n",
    "    if importlib.util.find_spec(package) is None:\n",
    "        !pip install {package}\n",
    "        \n",
    "for package in [\"tensorflow\", \"matplotlib\", \"numpy\", \"sklearn\", \"pandas\"]:\n",
    "    install_if_missing(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With the packages installed, we can now get started on the practical session!\n",
    "\n",
    "Today, we will be working with the famous MNIST dataset. MNIST (Modified National Institute of Standards and Technology) is a database of low resolution images of handwritten digits. The history here is interesting - the dataset was originally created in the 1980s, when researchers from the aforementioned institute collected samples from American Census Bureau employees and high school students. The dataset was then modified in the 1990s (hence the M in MNIST), and has since become a popular benchmark for machine learning algorithms. \n",
    "\n",
    "The dataset contains images, each of which is a 28x28 grayscale image of a handwritten digit. The goal is to classify each image into one of the 10 possible classes (0-9).\n",
    "\n",
    "![MNIST](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "The Scikit-Learn library provides a convenient function to download and load the MNIST dataset. The following cell will download the dataset. Then we will take a look at the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This means that we have 1797 images, each of which is a 8x8 image. For basic image processing, we will need to flatten the images into a 1D array. In this case, Scikit-Learn has already provided the data in this format too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For each image, we also have the corresponding label (or target, or class) in `digits.target`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can take a look at some random images from the dataset. The following cell will select 9 random images and plot them in a 3x3 grid (meaning that you can rerun the cell to see different images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selecting 9 random indices\n",
    "random_indices = np.random.choice(len(digits.images), 9, replace=False)\n",
    "\n",
    "# Creating a 3x3 grid plot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[random_indices[i]], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f\"Label: {digits.target[random_indices[i]]}\")\n",
    "\n",
    "    # Removing axis labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As you can see, these images are very low resolution. This is because they were originally scanned from paper forms, and then scaled down to 8x8 pixels. This is a common problem in machine learning - the quality of the data is often a limiting factor in the performance of the model. In this case, the low resolution of the images makes it difficult to distinguish between some digits, even for humans. For example, the following images are all labelled as 9, but they look very different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selecting 9 random indices of images labelled as 9\n",
    "random_indices = np.random.choice(np.where(digits.target == 9)[0], 9, replace=False)\n",
    "\n",
    "# Creating a 3x3 grid plot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[random_indices[i]], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f\"Label: {digits.target[random_indices[i]]}\")\n",
    "\n",
    "    # Removing axis labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "While we are plotting the samples as images, remember that our model is only going to see a 1D array of numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split\n",
    "\n",
    "In order to understand how well our model performs on _new_ data, we need to split our dataset into a training set and a test set. The training set will be used to train the model, and the test set will be used to evaluate the performance of the model.\n",
    "\n",
    "Let's keep some held-out data to be able to measure the generalization performance of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, \n",
    "    digits.target,\n",
    "    test_size=0.2, # 20% of the data is used for testing\n",
    "    random_state=42 # Providing a value here means getting the same \"random\" split every time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's confirm that the data has been split correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1437, 64)\n",
      "y_train shape: (1437,)\n",
      "X_test shape: (360, 64)\n",
      "y_test shape: (360,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is what we expected to see. It's always good to check as you go, to make sure that you haven't made a mistake somewhere - this is something that working in a notebook like this makes it easy to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the Target Data\n",
    "\n",
    "The labels that we have are integers between 0 and 9. However, we want to train a neural network to classify the images into one of 10 classes. It can be a little counter-intuitive because we are dealing with numbers, but our classes are not ordinal.\n",
    "\n",
    "What do we mean by that? Let's imagine we were trying to predict the height of a building (separated into classes) from images. If a given building was actually 10m tall, and our model predicted 9m, we would consider that to be a better prediction than if it predicted 1m. This is because the classes are ordinal - there is meaning in the difference between the classes.\n",
    "\n",
    "In our case, even though we are dealing with numbers, the classes are not ordinal. If a given image is actually a 9, and our model predicts 8, we would consider that to be just as bad as if it predicted 1. This is because the classes are not ordered, and the difference between the classes is not meaningful.\n",
    "\n",
    "Because of this, we need to convert our labels from an integer value into a one-hot encoded vector. This means that each label will be represented as a vector of length 10, with a 1 in the position corresponding to the class, and 0s everywhere else. For example, the label 9 would be represented as `[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`. This is a common way of representing categorical data in machine learning. By doing this, we ensure that our model is taught the correct relationship between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _pywrap_tf2: A dynamic link library (DLL) initialization routine failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBefore one-hot encoding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m y_train \u001b[38;5;241m=\u001b[39m to_categorical(y_train, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bash\\Documents\\GitHub\\deep_learning\\envdl\\Lib\\site-packages\\tensorflow\\__init__.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF2_BEHAVIOR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n",
      "File \u001b[1;32mc:\\Users\\bash\\Documents\\GitHub\\deep_learning\\envdl\\Lib\\site-packages\\tensorflow\\python\\tf2.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Tools to help with the TensorFlow 2.0 transition.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module is meant for TensorFlow internal implementation, not for users of\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mthe TensorFlow library. For that see tf.compat instead.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_tf2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menable\u001b[39m():\n\u001b[0;32m     26\u001b[0m   \u001b[38;5;66;03m# Enables v2 behaviors.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tf2: A dynamic link library (DLL) initialization routine failed."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(f'Before one-hot encoding: {y_train[0]}')\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "print(f'After one-hot encoding: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Networks with Keras\n",
    "\n",
    "Now that we have prepared our data, it's time to build a simple neural network! In this section, we will use the Keras API to build a simple feed forward neural network. We will then train the model on the MNIST dataset, and evaluate its performance on the test set.\n",
    "\n",
    "In most modern deep learning frameworks, the process of building a model can be broken down into a few steps:\n",
    "\n",
    "- Define the model architecture: this is where we define the layers of the model, and how they are connected to each other.\n",
    "- Compile the model: this is where we define the loss function, the optimizer, and the metrics that we want to use to evaluate the model.\n",
    "- Train the model: this is where we train the model on the training data.\n",
    "\n",
    "Let's start with defining the model architecture. There are two ways to do this in Keras - the Sequential API and the Functional API. The Sequential API is the simplest way to build a model, and is suitable for most use cases. The Functional API is more flexible, and allows you to build more complex models. We will start with the Sequential API, and then we will look at the Functional API later in the course.\n",
    "\n",
    "Our simple neural network will be \"fully-connected\". This means that each neuron in a given layer is connected to every neuron in the next layer. This is also known as a \"dense\" layer. We will use the `Dense` class from Keras to define our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(64, activation='relu', input_shape=(64,))) # 64 neurons, ReLU activation, input shape of 64\n",
    "\n",
    "# Hidden layer\n",
    "model.add(Dense(64, activation='relu')) # 64 neurons, ReLU activation\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(10, activation='softmax')) # 10 neurons, softmax activation\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Congratulations! You have just built your first neural network with Keras. As we can confirm from the `model.summary()` output, our model has 3 layers. The first layer has 64 neurons, the second layer has 64 neurons, and the output layer has 10 neurons. The output layer uses the softmax activation function, which is commonly used for multi-class classification problems. The other layers use the ReLU activation function, which is commonly used for hidden layers in neural networks.\n",
    "\n",
    "Next, we need to compile the model. This is where we define the loss function, the optimizer, and the metrics that we want to use to evaluate the model. We will use the `compile` method of the model to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy', # Loss function\n",
    "    optimizer='sgd', # Optimizer\n",
    "    metrics=['accuracy'] # Metrics to evaluate the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Because we are predicting which class a sample belongs to, we will use the `categorical_crossentropy` function. This loss function is commonly used for multi-class classification problems. \n",
    "\n",
    "For our optimizer, we are using the standard stochastic gradient descent (SGD) algorithm. This is a simple optimizer that works well for many problems. We will look at more advanced optimizers later in the course.\n",
    "\n",
    "Finally, we are using the `accuracy` metric to evaluate the model. This is a common metric for classification problems, and it is simply the fraction of samples that are correctly classified. This is an easier metric for us to understand, but it's not quite as useful for actually training the model (for example, it doesn't tell us how \"confident\" the model is in its predictions).\n",
    "\n",
    "Now that we have (a) defined the model architecture and (b) compiled the model, we are ready to train the model. We will use the `fit` method of the model to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train, # Training data\n",
    "    y_train, # Training labels\n",
    "    epochs=5, # Number of epochs\n",
    "    batch_size=32, # Number of samples per batch\n",
    "    validation_split=0.2 # Use 20% of the data for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We have now trained our model! We can see that the model has been trained for 5 epochs, and the loss and accuracy have been printed for each epoch. We can also see that the model has been evaluated on the validation data at the end of each epoch. This is useful for us to see how the model is performing on data that it hasn't seen during training.\n",
    "\n",
    "Once the model is trained, it's time to evaluate the model on the test set. We can use the `evaluate` method of the model to do this. If you were building a model for a real-world application, this is the very last thing you would do, and the result here would be the figure you'd report in your paper or presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Loss:     {loss:.2f}')\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hopefully you have achieved an accuracy of around 95%. This is pretty good, but we can do better! In the next section, we will look at how we can improve the performance of our model by using a more advanced optimizer. But before we get there, let's do one other thing - let's look at the predictions that our model is making on the test set. When you are building a model, it's often useful to have a look at some of the examples your model is getting wrong. Sometimes this can reveal problems with the data, or it can give you ideas for how to improve your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the predictions for the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get the index of the largest probability (i.e. the predicted class)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "misclassified_indices = np.where(predicted_classes != true_classes)[0]\n",
    "\n",
    "# Get the misclassified samples themselves\n",
    "misclassified_samples = X_test[misclassified_indices]\n",
    "misclassified_labels = np.argmax(y_test[misclassified_indices], axis=1)\n",
    "\n",
    "# Pick 9 random misclassified samples\n",
    "random_indices = np.random.choice(len(misclassified_indices), 9, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(misclassified_samples[random_indices[i]].reshape(8, 8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f\"Pred: {predicted_classes[misclassified_indices[random_indices[i]]]}, Real: {misclassified_labels[random_indices[i]]}\")\n",
    "\n",
    "    # Removing axis labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What do you think? Would you have made the same mistakes as the model? Determining whether the mistakes are \"understandable\" is a rough way of seeing if you could improve the model further, or if this is the best you can do with the data you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Exercises: Impact of the Optimizer\n",
    "\n",
    "In this section, you will play around with the optimizer and see how it affects the performance of the model. We will start with the standard SGD optimizer, and then we will look at more advanced optimizers.\n",
    "\n",
    "1. Try decreasing the learning rate of the SGD optimizer by a factor of 10, or 100. What do you observe?\n",
    "2. Try increasing the learning rate of the SGD optimizer. What happens?\n",
    "3. The SGD optimizer has a momentum parameter. In a nutshell, this parameter controls how much the gradient from the previous step affects the current step. Try enabling momentum in the SGD optimizer with a value of 0.9. What happens?\n",
    "  \n",
    "**Notes**: \n",
    "\n",
    "The keras API documentation is available at:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "\n",
    "It is also possible to learn more about the parameters of a class by using the question mark: type and evaluate:\n",
    "\n",
    "```python\n",
    "optimizers.SGD?\n",
    "```\n",
    "\n",
    "in a jupyter notebook cell.\n",
    "\n",
    "It is also possible to type the beginning of a function call / constructor and type \"shift-tab\" after the opening paren:\n",
    "\n",
    "```python\n",
    "optimizers.SGD(<shift-tab>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - copy the relevant parts from the previous section and add more cells as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try a more advanced optimizer. Adam is likely the most popular optimizer for deep learning. It is an adaptive learning rate optimizer, which means that it automatically adjusts the learning rate based on how the training is going. This can be very useful, as it means that we don't need to manually tune the learning rate. Let's see how it performs on our model.\n",
    "\n",
    "\n",
    "1. Replace the SGD optimizer by the Adam optimizer from keras and run it\n",
    "  with the default parameters.\n",
    "\n",
    "2. Add another hidden layer with ReLU activation and 64 neurons. Does it improve the model performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises: Forward Pass and Generalization\n",
    "\n",
    "Let's look in more detail at how the model makes predictions on the test set. We will walk through each step of making predictions, examining exactly what's going on.\n",
    "\n",
    "To start, we will apply our model to the test set, and look at what we get as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tf = model(X_test)\n",
    "predictions_tf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions_tf), predictions_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output of the model is a tensor of shape `(360, 10)`. This means that we have 360 samples, and for each sample we have 10 values. Each of these values represents the probability that the sample belongs to a given class. This means that we have 10 probabilities for each sample, and the sum of these probabilities is 1. We can confirm this by summing the probabilities for each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reduce_sum(predictions_tf, axis=1)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "...okay, there might be a small rounding error here and there. This is to do with how floating point numbers are represented in computers, and it's not something we need to worry about for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract the label with the highest probability using the tensorflow API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_tf = tf.argmax(predictions_tf, axis=1)\n",
    "predicted_labels_tf[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One helpful aspect of this approach is that we don't just get the prediction, but also a sense of how confident the model is in its prediction. To see this in practice, let's take a look at some of the predictions the model is highly confident about (i.e. a lot of the probability mass is on one class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the values corresponding to the predicted labels for each sample\n",
    "predicted_values_tf = tf.reduce_max(predictions_tf, axis=1)\n",
    "\n",
    "# Get the indices of the samples with the highest predicted values\n",
    "most_confident_indices_tf = tf.argsort(predicted_values_tf, direction='DESCENDING').numpy()[:9]\n",
    "\n",
    "# Get the 9 most confident samples\n",
    "most_confident_samples_tf = X_test[most_confident_indices_tf]\n",
    "\n",
    "# Get the true labels for the 9 most confident samples\n",
    "most_confident_labels_tf = np.argmax(y_test[most_confident_indices_tf], axis=1)\n",
    "\n",
    "# Plot the 9 most confident samples\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(most_confident_samples_tf[i].reshape(8, 8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f\"{most_confident_labels_tf[i]}\")\n",
    "\n",
    "    # Removing axis labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Initialization\n",
    "\n",
    "Let's study the impact of a bad initialization when training\n",
    "a deep feed forward network.\n",
    "\n",
    "By default, Keras dense layers use the \"Glorot Uniform\" initialization\n",
    "strategy to initialize the weight matrices:\n",
    "\n",
    "- each weight coefficient is randomly sampled from [-scale, scale]\n",
    "- scale is proportional to $\\frac{1}{\\sqrt{n_{in} + n_{out}}}$\n",
    "\n",
    "This strategy is known to work well to initialize deep neural networks\n",
    "with \"tanh\" or \"relu\" activation functions and then trained with\n",
    "standard SGD.\n",
    "\n",
    "To assess the impact of initialization let us plug an alternative init\n",
    "scheme into a 2 hidden layers networks with \"tanh\" activations.\n",
    "For the sake of the example let's use normal distributed weights\n",
    "with a manually adjustable scale (standard deviation) and see the\n",
    "impact the scale value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "input_dim = 64\n",
    "hidden_dim = 64\n",
    "output_dim = 10\n",
    "\n",
    "normal_init = initializers.TruncatedNormal(stddev=0.01, seed=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_dim, input_dim=input_dim, activation=\"tanh\",\n",
    "                kernel_initializer=normal_init))\n",
    "model.add(Dense(hidden_dim, activation=\"tanh\",\n",
    "                kernel_initializer=normal_init))\n",
    "model.add(Dense(output_dim, activation=\"softmax\",\n",
    "                kernel_initializer=normal_init))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=0.1),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the parameters of the first layer after initialization but before any training has happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.layers[0].weights[0].numpy()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model.layers[0].weights[1].numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=15, batch_size=32)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['loss'], label=\"Truncated Normal init\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been fit, the weights have been updated and notably the biases are no longer 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "- Try the following initialization schemes and see whether\n",
    "  the SGD algorithm can successfully train the network or\n",
    "  not:\n",
    "  \n",
    "  - a very small e.g. `stddev=1e-3`\n",
    "  - a larger scale e.g. `stddev=1` or `10`\n",
    "  - initialize all weights to 0 (constant initialization)\n",
    "  \n",
    "- What do you observe? Can you find an explanation for those\n",
    "  outcomes?\n",
    "\n",
    "- Are more advanced solvers such as SGD with momentum or Adam able\n",
    "  to deal better with such bad initializations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
